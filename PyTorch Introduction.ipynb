{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Introduction\n",
    "## PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n",
    "### Can be seen as substitute of NumPy with GPU capabilities\n",
    "A Tensor is just a more generic term than matrix or vector.\n",
    "PyTorch Tensors There appear to be 4 major types of tensors in PyTorch: Byte, Float, Double, and Long tensors. Each tensor type corresponds to the type of number (and more importantly the size/preision of the number) contained in each place of the matrix.\n",
    "\n",
    "\n",
    "## NumPy:\n",
    "NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n",
    "* For Numpy docs: Refer the following https://docs.scipy.org/doc/numpy-1.13.0/reference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n",
    "A very similar package for `numpy.ndarray`. It basically supports almost every major computation of numpy\n",
    "\n",
    "### Lets dive in and see some basic pythonic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.arange(start,end,step=1) -> [start,end) with step\n",
    "x = torch.arange(0,3,step=0.5)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.FloatTensor(size or list)\n",
    "x = torch.FloatTensor(2,3)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NumPy to PyTorch and vice-versa\n",
    "With almost no computation cost, you can convert PyTorch tensor to NumPy array and any change in the converted NumPy array will reflect on the original PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# torch.from_numpy(ndarray) -> tensor\n",
    "\n",
    "x1 = np.ndarray(shape=(2,3), dtype=int,buffer=np.array([1,2,3,4,5,6]))\n",
    "x2 = torch.from_numpy(x1)\n",
    "\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor.numpy() -> ndarray\n",
    "x3 = x2.numpy()\n",
    "x3\n",
    "\n",
    "# Defining a numpy array and converting it to a Torch tensor\n",
    "# a = np.ndarray(shape=(2,3), dtype=float)\n",
    "# a = torch.from_numpy(a)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_gpu = x.cuda()\n",
    "# x_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor.size() -> indexing also possible\n",
    "\n",
    "x = torch.FloatTensor(10,12,3,3)\n",
    "\n",
    "x.size()[:]\n",
    "\n",
    "# x.size()[:2]\n",
    "# x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The contents of a tensor can be accessed and modified using Python’s indexing and slicing notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.index_select(input, dim, index)\n",
    "\n",
    "x = torch.rand(4,3)\n",
    "out = torch.index_select(x,0,torch.LongTensor([0,3]))\n",
    "\n",
    "x,out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pythonic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pythonic indexing also works\n",
    "\n",
    "x[:,0],x[0,:],x[0:2,0:2]\n",
    "\n",
    "# name = 'abhishek'\n",
    "# name = name[0:2]\n",
    "# name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.masked_select(input, mask)\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "mask = torch.ByteTensor([[0,0,1],[0,1,0]])\n",
    "out = torch.masked_select(x,mask)\n",
    "\n",
    "x, mask, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat(seq, dim=0) -> concatenate tensor along dim\n",
    "\n",
    "x = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "y = torch.FloatTensor([[-1,-2,-3],[-4,-5,-6]])\n",
    "z1 = torch.cat([x,y],dim=0)\n",
    "z2 = torch.cat([x,y],dim=1)\n",
    "\n",
    "x,y,z1,z2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Functions\n",
    "\n",
    "### Torch provides MATLAB-like functions for manipulating Tensor objects. \n",
    "\n",
    "`torch.add(tensor, value)`\n",
    "Add the given value to all elements in the `Tensor`.\n",
    "\n",
    "\n",
    "`y = torch.add(x, value)` returns a new `Tensor`.\n",
    "\n",
    "`x:add(value)` add `value` to all elements in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.add()\n",
    "\n",
    "x1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "x2 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
    "add = torch.add(x1,x2)\n",
    "\n",
    "x1,x2,add,x1+x2,x1-x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix matrix product of `mat1` and `mat2`. \n",
    "If mat1 is a n × m matrix, mat2 a m × p matrix, res must be a n × p matrix.\n",
    "\n",
    "`torch.mm(x, y)` puts the result in a new Tensor.\n",
    "\n",
    "`torch.mm(M, x, y)` puts the result in M.\n",
    "\n",
    "`M:mm(x, y)` puts the result in M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mm(mat1, mat2) -> matrix multiplication\n",
    "\n",
    "x1 = torch.FloatTensor(3,4)\n",
    "x2 = torch.FloatTensor(4,5)\n",
    "\n",
    "torch.mm(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.eig(a,eigenvectors=False) -> eigen_value, eigen_vector\n",
    "\n",
    "x1 = torch.FloatTensor(4,4)\n",
    "\n",
    "x1 = torch.eig(x1,True)\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd is now a core torch package for automatic differentiation. It uses a tape based system for automatic differentiation.\n",
    "\n",
    "In autograd, there is a Variable class, which is a very thin wrapper around a Tensor. \n",
    "You can access the raw tensor through the `.data attribute`, and after computing the backward pass, a gradient w.r.t. this variable is accumulated into `.grad attribute`.\n",
    "\n",
    "#### We wrap our PyTorch Tensors in Variable objects; a Variable represents a node in a computational graph. If x is a Variable then `x.data` is a Tensor, and `x.grad` is another Variable holding the gradient of x with respect to some scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "Setting `requires_grad=False` indicates that we do not need to compute gradients with respect to these Variables during the backward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Create random Tensors for weights, and wrap them in Variables.\n",
    "#### Setting requires_grad=True indicates that we want to compute gradients with respect to these Variables during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass: \n",
    "compute predicted y using operations on Variables\n",
    "\n",
    "\n",
    "### Use autograd:\n",
    "to compute the backward pass. This call will compute the gradient of loss with respect to all Variables with requires_grad=True.\n",
    "\n",
    "After this call w1.grad and w2.grad will be Variables holding the gradient of the loss with respect to w1 and w2 respectively.\n",
    "\n",
    "\n",
    "### Update weights:\n",
    "using gradient descent; w1.data and w2.data are Tensors, w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are Tensors.\n",
    "\n",
    "#### Manually zero the gradients after running the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss using operations on Variables.\n",
    "  # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "  # (1,); loss.data[0] is a scalar value holding the loss.\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.data[0])\n",
    "\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  w1.data -= learning_rate * w1.grad.data\n",
    "  w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "  # Manually zero the gradients after running the backward pass\n",
    "  w1.grad.data.zero_()\n",
    "  w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch NN module\n",
    "\n",
    "The nn package defines a set of Modules, which you can think of as a neural network layer that has produces output from input and may have some trainable weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the nn package to define our model as a sequence of layers.\n",
    "nn.Sequential is a Module which contains other Modules, and applies them in sequence to produce its output. \n",
    "Each Linear Module computes output from input using a linear function, and holds internal Variables for its weight and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function:\n",
    "The nn package also contains definitions of popular loss functions; in this case we will use Mean Squared Error (MSE) as our loss function.\n",
    "We'll also initialise the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization:\n",
    "Use the optim package to define an Optimizer that will update the weights of the model for us. \n",
    "\n",
    "Here we will use Adam; the optim package contains many otheroptimization algoriths. \n",
    "The first argument to the Adam constructor tells the optimizer which Variables it should update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass: \n",
    "compute predicted y by passing x to the model. Module objects override the __call__ operator so you can call them like functions. \n",
    "When doing so you pass a Variable of input data to the Module and it produces a Variable of output data.\n",
    "\n",
    "### Compute Loss\n",
    "\n",
    "### Zero the gradients before running the backward pass.\n",
    "\n",
    "### Backward pass: \n",
    "compute gradient of the loss with respect to all the learnable parameters of the model. Internally, the parameters of each Module are stored in Variables with requires_grad=True, so this call will compute gradients for all learnable parameters in the model.\n",
    "\n",
    "### Update the weights using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss.\n",
    "  loss = loss_fn(y_pred, y)\n",
    "  print(t, loss.data[0])\n",
    "  \n",
    "  # Before the backward pass, use the optimizer object to zero all of the\n",
    "  # gradients for the variables it will update (which are the learnable weights\n",
    "  # of the model)\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "  loss.backward()\n",
    "\n",
    "  # Calling the step function on an Optimizer makes an update to its parameters\n",
    "  optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
